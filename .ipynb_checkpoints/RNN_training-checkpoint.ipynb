{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanda\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "text = open(os.path.join(os.getcwd(), 'data/shakespeare.txt')).read()\n",
    "\n",
    "# remove sonnet numbers and convert to lowercase\n",
    "text = re.sub(r'[0-9]+', '', text) \n",
    "text = text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(text) # total length of dataset\n",
    "n_vocab = len(chars) # number of unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 40\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# use sliding window approach\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    seq_out = text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    \n",
    "n_sequences = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_sequences, seq_length, 1))\n",
    "\n",
    "# normalize data to range (0, 1)\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model \n",
    "# (single layer with 150 units, followed by dense output layer)\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# start training from previous checkpoint\n",
    "file = \"weights-improvement-20-1.7396.hdf5\"\n",
    "model.load_weights(file)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "97635/97635 [==============================] - 78s 795us/step - loss: 1.7454\n",
      "\n",
      "Epoch 00001: loss improved from 2.90348 to 1.74536, saving model to weights-improvement-01-1.7454.hdf5\n",
      "Epoch 2/200\n",
      "97635/97635 [==============================] - 84s 861us/step - loss: 1.7397\n",
      "\n",
      "Epoch 00002: loss improved from 1.74536 to 1.73969, saving model to weights-improvement-02-1.7397.hdf5\n",
      "Epoch 3/200\n",
      "97635/97635 [==============================] - 92s 944us/step - loss: 1.7379\n",
      "\n",
      "Epoch 00003: loss improved from 1.73969 to 1.73791, saving model to weights-improvement-03-1.7379.hdf5\n",
      "Epoch 4/200\n",
      "97635/97635 [==============================] - 89s 909us/step - loss: 1.7367\n",
      "\n",
      "Epoch 00004: loss improved from 1.73791 to 1.73667, saving model to weights-improvement-04-1.7367.hdf5\n",
      "Epoch 5/200\n",
      "97635/97635 [==============================] - 83s 855us/step - loss: 1.7366\n",
      "\n",
      "Epoch 00005: loss improved from 1.73667 to 1.73661, saving model to weights-improvement-05-1.7366.hdf5\n",
      "Epoch 6/200\n",
      "97635/97635 [==============================] - 77s 785us/step - loss: 1.7313\n",
      "\n",
      "Epoch 00006: loss improved from 1.73661 to 1.73127, saving model to weights-improvement-06-1.7313.hdf5\n",
      "Epoch 7/200\n",
      "97635/97635 [==============================] - 80s 824us/step - loss: 1.7301\n",
      "\n",
      "Epoch 00007: loss improved from 1.73127 to 1.73006, saving model to weights-improvement-07-1.7301.hdf5\n",
      "Epoch 8/200\n",
      "97635/97635 [==============================] - 80s 822us/step - loss: 1.7298\n",
      "\n",
      "Epoch 00008: loss improved from 1.73006 to 1.72983, saving model to weights-improvement-08-1.7298.hdf5\n",
      "Epoch 9/200\n",
      "97635/97635 [==============================] - ETA: 0s - loss: 1.731 - 80s 821us/step - loss: 1.7310\n",
      "\n",
      "Epoch 00009: loss did not improve from 1.72983\n",
      "Epoch 10/200\n",
      "97635/97635 [==============================] - 88s 897us/step - loss: 1.7240\n",
      "\n",
      "Epoch 00010: loss improved from 1.72983 to 1.72396, saving model to weights-improvement-10-1.7240.hdf5\n",
      "Epoch 11/200\n",
      "97635/97635 [==============================] - 85s 869us/step - loss: 1.7234\n",
      "\n",
      "Epoch 00011: loss improved from 1.72396 to 1.72340, saving model to weights-improvement-11-1.7234.hdf5\n",
      "Epoch 12/200\n",
      "97635/97635 [==============================] - 95s 978us/step - loss: 1.7257\n",
      "\n",
      "Epoch 00012: loss did not improve from 1.72340\n",
      "Epoch 13/200\n",
      "97635/97635 [==============================] - 84s 864us/step - loss: 1.7194\n",
      "\n",
      "Epoch 00013: loss improved from 1.72340 to 1.71939, saving model to weights-improvement-13-1.7194.hdf5\n",
      "Epoch 14/200\n",
      "97635/97635 [==============================] - 78s 802us/step - loss: 1.7168\n",
      "\n",
      "Epoch 00014: loss improved from 1.71939 to 1.71681, saving model to weights-improvement-14-1.7168.hdf5\n",
      "Epoch 15/200\n",
      "97635/97635 [==============================] - 78s 798us/step - loss: 1.7178\n",
      "\n",
      "Epoch 00015: loss did not improve from 1.71681\n",
      "Epoch 16/200\n",
      "97635/97635 [==============================] - 86s 879us/step - loss: 1.7151\n",
      "\n",
      "Epoch 00016: loss improved from 1.71681 to 1.71514, saving model to weights-improvement-16-1.7151.hdf5\n",
      "Epoch 17/200\n",
      "97635/97635 [==============================] - ETA: 0s - loss: 1.712 - 97s 998us/step - loss: 1.7122\n",
      "\n",
      "Epoch 00017: loss improved from 1.71514 to 1.71222, saving model to weights-improvement-17-1.7122.hdf5\n",
      "Epoch 18/200\n",
      "97635/97635 [==============================] - 80s 821us/step - loss: 1.7125\n",
      "\n",
      "Epoch 00018: loss did not improve from 1.71222\n",
      "Epoch 19/200\n",
      "97635/97635 [==============================] - 77s 786us/step - loss: 1.7108\n",
      "\n",
      "Epoch 00019: loss improved from 1.71222 to 1.71080, saving model to weights-improvement-19-1.7108.hdf5\n",
      "Epoch 20/200\n",
      "97635/97635 [==============================] - 78s 794us/step - loss: 1.7079\n",
      "\n",
      "Epoch 00020: loss improved from 1.71080 to 1.70794, saving model to weights-improvement-20-1.7079.hdf5\n",
      "Epoch 21/200\n",
      "97635/97635 [==============================] - 76s 784us/step - loss: 1.7058\n",
      "\n",
      "Epoch 00021: loss improved from 1.70794 to 1.70578, saving model to weights-improvement-21-1.7058.hdf5\n",
      "Epoch 22/200\n",
      "97635/97635 [==============================] - 78s 797us/step - loss: 1.7036\n",
      "\n",
      "Epoch 00023: loss improved from 1.70371 to 1.70359, saving model to weights-improvement-23-1.7036.hdf5\n",
      "Epoch 24/200\n",
      "97635/97635 [==============================] - 77s 786us/step - loss: 1.7024\n",
      "\n",
      "Epoch 00024: loss improved from 1.70359 to 1.70241, saving model to weights-improvement-24-1.7024.hdf5\n",
      "Epoch 25/200\n",
      "97635/97635 [==============================] - 77s 793us/step - loss: 1.7028\n",
      "\n",
      "Epoch 00025: loss did not improve from 1.70241\n",
      "Epoch 26/200\n",
      "97635/97635 [==============================] - 77s 790us/step - loss: 1.6985\n",
      "\n",
      "Epoch 00026: loss improved from 1.70241 to 1.69849, saving model to weights-improvement-26-1.6985.hdf5\n",
      "Epoch 27/200\n",
      "97635/97635 [==============================] - 77s 790us/step - loss: 1.6964\n",
      "\n",
      "Epoch 00027: loss improved from 1.69849 to 1.69643, saving model to weights-improvement-27-1.6964.hdf5\n",
      "Epoch 28/200\n",
      "97635/97635 [==============================] - 77s 787us/step - loss: 1.6967\n",
      "\n",
      "Epoch 00028: loss did not improve from 1.69643\n",
      "Epoch 29/200\n",
      "97635/97635 [==============================] - 77s 787us/step - loss: 1.6950\n",
      "\n",
      "Epoch 00029: loss improved from 1.69643 to 1.69495, saving model to weights-improvement-29-1.6950.hdf5\n",
      "Epoch 30/200\n",
      "97635/97635 [==============================] - 77s 786us/step - loss: 1.6957\n",
      "\n",
      "Epoch 00030: loss did not improve from 1.69495\n",
      "Epoch 31/200\n",
      "97635/97635 [==============================] - 77s 789us/step - loss: 1.6874\n",
      "\n",
      "Epoch 00031: loss improved from 1.69495 to 1.68737, saving model to weights-improvement-31-1.6874.hdf5\n",
      "Epoch 32/200\n",
      "97635/97635 [==============================] - 82s 842us/step - loss: 1.6908\n",
      "\n",
      "Epoch 00032: loss did not improve from 1.68737\n",
      "Epoch 33/200\n",
      "97635/97635 [==============================] - 78s 799us/step - loss: 1.6885\n",
      "\n",
      "Epoch 00033: loss did not improve from 1.68737\n",
      "Epoch 34/200\n",
      "97635/97635 [==============================] - 87s 895us/step - loss: 1.6829\n",
      "\n",
      "Epoch 00034: loss improved from 1.68737 to 1.68295, saving model to weights-improvement-34-1.6829.hdf5\n",
      "Epoch 35/200\n",
      "97635/97635 [==============================] - 83s 850us/step - loss: 1.6867\n",
      "\n",
      "Epoch 00035: loss did not improve from 1.68295\n",
      "Epoch 36/200\n",
      "97635/97635 [==============================] - 93s 954us/step - loss: 1.6797\n",
      "\n",
      "Epoch 00036: loss improved from 1.68295 to 1.67966, saving model to weights-improvement-36-1.6797.hdf5\n",
      "Epoch 37/200\n",
      "97635/97635 [==============================] - 87s 888us/step - loss: 1.6819\n",
      "\n",
      "Epoch 00037: loss did not improve from 1.67966\n",
      "Epoch 38/200\n",
      "97635/97635 [==============================] - 79s 812us/step - loss: 1.6816\n",
      "\n",
      "Epoch 00038: loss did not improve from 1.67966\n",
      "Epoch 39/200\n",
      "97635/97635 [==============================] - 78s 804us/step - loss: 1.6800\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.67966\n",
      "Epoch 40/200\n",
      "97635/97635 [==============================] - 77s 785us/step - loss: 1.6781\n",
      "\n",
      "Epoch 00040: loss improved from 1.67966 to 1.67808, saving model to weights-improvement-40-1.6781.hdf5\n",
      "Epoch 41/200\n",
      "97635/97635 [==============================] - 94s 967us/step - loss: 1.6738\n",
      "\n",
      "Epoch 00041: loss improved from 1.67808 to 1.67385, saving model to weights-improvement-41-1.6738.hdf5\n",
      "Epoch 42/200\n",
      "97635/97635 [==============================] - 86s 876us/step - loss: 1.6756\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.67385\n",
      "Epoch 43/200\n",
      "97635/97635 [==============================] - 88s 900us/step - loss: 1.6775\n",
      "\n",
      "Epoch 00043: loss did not improve from 1.67385\n",
      "Epoch 44/200\n",
      "97635/97635 [==============================] - 84s 861us/step - loss: 1.6729\n",
      "\n",
      "Epoch 00044: loss improved from 1.67385 to 1.67295, saving model to weights-improvement-44-1.6729.hdf5\n",
      "Epoch 45/200\n",
      "71296/97635 [====================>.........] - ETA: 22s - loss: 1.6621"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(X, y, epochs=200, batch_size=128, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
