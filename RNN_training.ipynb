{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanda\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\amanda\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "text = open(os.path.join(os.getcwd(), 'data/shakespeare.txt')).read()\n",
    "\n",
    "# remove sonnet numbers and convert to lowercase\n",
    "text = re.sub(r'[0-9]+', '', text) \n",
    "text = text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(text) # total length of dataset\n",
    "n_vocab = len(chars) # number of unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 40\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# use sliding window approach\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    seq_out = text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    \n",
    "n_sequences = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_sequences, seq_length, 1))\n",
    "\n",
    "# normalize data to range (0, 1)\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model \n",
    "# (single layer with 150 units, followed by dense output layer)\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# start training from previous checkpoint\n",
    "file = \"weights-improvement-20-1.7396.hdf5\"\n",
    "model.load_weights(file)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "97635/97635 [==============================] - 78s 795us/step - loss: 1.7454\n",
      "\n",
      "Epoch 00001: loss improved from 2.90348 to 1.74536, saving model to weights-improvement-01-1.7454.hdf5\n",
      "Epoch 2/200\n",
      "97635/97635 [==============================] - 84s 861us/step - loss: 1.7397\n",
      "\n",
      "Epoch 00002: loss improved from 1.74536 to 1.73969, saving model to weights-improvement-02-1.7397.hdf5\n",
      "Epoch 3/200\n",
      "97635/97635 [==============================] - 92s 944us/step - loss: 1.7379\n",
      "\n",
      "Epoch 00003: loss improved from 1.73969 to 1.73791, saving model to weights-improvement-03-1.7379.hdf5\n",
      "Epoch 4/200\n",
      "97635/97635 [==============================] - 89s 909us/step - loss: 1.7367\n",
      "\n",
      "Epoch 00004: loss improved from 1.73791 to 1.73667, saving model to weights-improvement-04-1.7367.hdf5\n",
      "Epoch 5/200\n",
      "97635/97635 [==============================] - 83s 855us/step - loss: 1.7366\n",
      "\n",
      "Epoch 00005: loss improved from 1.73667 to 1.73661, saving model to weights-improvement-05-1.7366.hdf5\n",
      "Epoch 6/200\n",
      "97635/97635 [==============================] - 77s 785us/step - loss: 1.7313\n",
      "\n",
      "Epoch 00006: loss improved from 1.73661 to 1.73127, saving model to weights-improvement-06-1.7313.hdf5\n",
      "Epoch 7/200\n",
      "97635/97635 [==============================] - 80s 824us/step - loss: 1.7301\n",
      "\n",
      "Epoch 00007: loss improved from 1.73127 to 1.73006, saving model to weights-improvement-07-1.7301.hdf5\n",
      "Epoch 8/200\n",
      "97635/97635 [==============================] - 80s 822us/step - loss: 1.7298\n",
      "\n",
      "Epoch 00008: loss improved from 1.73006 to 1.72983, saving model to weights-improvement-08-1.7298.hdf5\n",
      "Epoch 9/200\n",
      "97635/97635 [==============================] - ETA: 0s - loss: 1.731 - 80s 821us/step - loss: 1.7310\n",
      "\n",
      "Epoch 00009: loss did not improve from 1.72983\n",
      "Epoch 10/200\n",
      "97635/97635 [==============================] - 88s 897us/step - loss: 1.7240\n",
      "\n",
      "Epoch 00010: loss improved from 1.72983 to 1.72396, saving model to weights-improvement-10-1.7240.hdf5\n",
      "Epoch 11/200\n",
      "97635/97635 [==============================] - 85s 869us/step - loss: 1.7234\n",
      "\n",
      "Epoch 00011: loss improved from 1.72396 to 1.72340, saving model to weights-improvement-11-1.7234.hdf5\n",
      "Epoch 12/200\n",
      "97635/97635 [==============================] - 95s 978us/step - loss: 1.7257\n",
      "\n",
      "Epoch 00012: loss did not improve from 1.72340\n",
      "Epoch 13/200\n",
      "97635/97635 [==============================] - 84s 864us/step - loss: 1.7194\n",
      "\n",
      "Epoch 00013: loss improved from 1.72340 to 1.71939, saving model to weights-improvement-13-1.7194.hdf5\n",
      "Epoch 14/200\n",
      "97635/97635 [==============================] - 78s 802us/step - loss: 1.7168\n",
      "\n",
      "Epoch 00014: loss improved from 1.71939 to 1.71681, saving model to weights-improvement-14-1.7168.hdf5\n",
      "Epoch 15/200\n",
      "97635/97635 [==============================] - 78s 798us/step - loss: 1.7178\n",
      "\n",
      "Epoch 00015: loss did not improve from 1.71681\n",
      "Epoch 16/200\n",
      "97635/97635 [==============================] - 86s 879us/step - loss: 1.7151\n",
      "\n",
      "Epoch 00016: loss improved from 1.71681 to 1.71514, saving model to weights-improvement-16-1.7151.hdf5\n",
      "Epoch 17/200\n",
      "97635/97635 [==============================] - ETA: 0s - loss: 1.712 - 97s 998us/step - loss: 1.7122\n",
      "\n",
      "Epoch 00017: loss improved from 1.71514 to 1.71222, saving model to weights-improvement-17-1.7122.hdf5\n",
      "Epoch 18/200\n",
      "97635/97635 [==============================] - 80s 821us/step - loss: 1.7125\n",
      "\n",
      "Epoch 00018: loss did not improve from 1.71222\n",
      "Epoch 19/200\n",
      "97635/97635 [==============================] - 77s 786us/step - loss: 1.7108\n",
      "\n",
      "Epoch 00019: loss improved from 1.71222 to 1.71080, saving model to weights-improvement-19-1.7108.hdf5\n",
      "Epoch 20/200\n",
      "97635/97635 [==============================] - 78s 794us/step - loss: 1.7079\n",
      "\n",
      "Epoch 00020: loss improved from 1.71080 to 1.70794, saving model to weights-improvement-20-1.7079.hdf5\n",
      "Epoch 21/200\n",
      "97635/97635 [==============================] - 76s 784us/step - loss: 1.7058\n",
      "\n",
      "Epoch 00021: loss improved from 1.70794 to 1.70578, saving model to weights-improvement-21-1.7058.hdf5\n",
      "Epoch 22/200\n",
      "97635/97635 [==============================] - 78s 797us/step - loss: 1.7036\n",
      "\n",
      "Epoch 00023: loss improved from 1.70371 to 1.70359, saving model to weights-improvement-23-1.7036.hdf5\n",
      "Epoch 24/200\n",
      "97635/97635 [==============================] - 77s 786us/step - loss: 1.7024\n",
      "\n",
      "Epoch 00024: loss improved from 1.70359 to 1.70241, saving model to weights-improvement-24-1.7024.hdf5\n",
      "Epoch 25/200\n",
      "97635/97635 [==============================] - 77s 793us/step - loss: 1.7028\n",
      "\n",
      "Epoch 00025: loss did not improve from 1.70241\n",
      "Epoch 26/200\n",
      "97635/97635 [==============================] - 77s 790us/step - loss: 1.6985\n",
      "\n",
      "Epoch 00026: loss improved from 1.70241 to 1.69849, saving model to weights-improvement-26-1.6985.hdf5\n",
      "Epoch 27/200\n",
      "97635/97635 [==============================] - 77s 790us/step - loss: 1.6964\n",
      "\n",
      "Epoch 00027: loss improved from 1.69849 to 1.69643, saving model to weights-improvement-27-1.6964.hdf5\n",
      "Epoch 28/200\n",
      "97635/97635 [==============================] - 77s 787us/step - loss: 1.6967\n",
      "\n",
      "Epoch 00028: loss did not improve from 1.69643\n",
      "Epoch 29/200\n",
      "97635/97635 [==============================] - 77s 787us/step - loss: 1.6950\n",
      "\n",
      "Epoch 00029: loss improved from 1.69643 to 1.69495, saving model to weights-improvement-29-1.6950.hdf5\n",
      "Epoch 30/200\n",
      "97635/97635 [==============================] - 77s 786us/step - loss: 1.6957\n",
      "\n",
      "Epoch 00030: loss did not improve from 1.69495\n",
      "Epoch 31/200\n",
      "97635/97635 [==============================] - 77s 789us/step - loss: 1.6874\n",
      "\n",
      "Epoch 00031: loss improved from 1.69495 to 1.68737, saving model to weights-improvement-31-1.6874.hdf5\n",
      "Epoch 32/200\n",
      "97635/97635 [==============================] - 82s 842us/step - loss: 1.6908\n",
      "\n",
      "Epoch 00032: loss did not improve from 1.68737\n",
      "Epoch 33/200\n",
      "97635/97635 [==============================] - 78s 799us/step - loss: 1.6885\n",
      "\n",
      "Epoch 00033: loss did not improve from 1.68737\n",
      "Epoch 34/200\n",
      "97635/97635 [==============================] - 87s 895us/step - loss: 1.6829\n",
      "\n",
      "Epoch 00034: loss improved from 1.68737 to 1.68295, saving model to weights-improvement-34-1.6829.hdf5\n",
      "Epoch 35/200\n",
      "97635/97635 [==============================] - 83s 850us/step - loss: 1.6867\n",
      "\n",
      "Epoch 00035: loss did not improve from 1.68295\n",
      "Epoch 36/200\n",
      "97635/97635 [==============================] - 93s 954us/step - loss: 1.6797\n",
      "\n",
      "Epoch 00036: loss improved from 1.68295 to 1.67966, saving model to weights-improvement-36-1.6797.hdf5\n",
      "Epoch 37/200\n",
      "97635/97635 [==============================] - 87s 888us/step - loss: 1.6819\n",
      "\n",
      "Epoch 00037: loss did not improve from 1.67966\n",
      "Epoch 38/200\n",
      "97635/97635 [==============================] - 79s 812us/step - loss: 1.6816\n",
      "\n",
      "Epoch 00038: loss did not improve from 1.67966\n",
      "Epoch 39/200\n",
      "97635/97635 [==============================] - 78s 804us/step - loss: 1.6800\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.67966\n",
      "Epoch 40/200\n",
      "97635/97635 [==============================] - 77s 785us/step - loss: 1.6781\n",
      "\n",
      "Epoch 00040: loss improved from 1.67966 to 1.67808, saving model to weights-improvement-40-1.6781.hdf5\n",
      "Epoch 41/200\n",
      "97635/97635 [==============================] - 94s 967us/step - loss: 1.6738\n",
      "\n",
      "Epoch 00041: loss improved from 1.67808 to 1.67385, saving model to weights-improvement-41-1.6738.hdf5\n",
      "Epoch 42/200\n",
      "97635/97635 [==============================] - 86s 876us/step - loss: 1.6756\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.67385\n",
      "Epoch 43/200\n",
      "97635/97635 [==============================] - 88s 900us/step - loss: 1.6775\n",
      "\n",
      "Epoch 00043: loss did not improve from 1.67385\n",
      "Epoch 44/200\n",
      "97635/97635 [==============================] - 84s 861us/step - loss: 1.6729\n",
      "\n",
      "Epoch 00044: loss improved from 1.67385 to 1.67295, saving model to weights-improvement-44-1.6729.hdf5\n",
      "Epoch 45/200\n",
      "97635/97635 [==============================] - 85s 873us/step - loss: 1.6751\n",
      "\n",
      "Epoch 00045: loss did not improve from 1.67295\n",
      "Epoch 46/200\n",
      "97635/97635 [==============================] - 85s 875us/step - loss: 1.6673\n",
      "\n",
      "Epoch 00046: loss improved from 1.67295 to 1.66732, saving model to weights-improvement-46-1.6673.hdf5\n",
      "Epoch 47/200\n",
      "97635/97635 [==============================] - 84s 859us/step - loss: 1.6682\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.66732\n",
      "Epoch 48/200\n",
      "97635/97635 [==============================] - 80s 824us/step - loss: 1.6665\n",
      "\n",
      "Epoch 00048: loss improved from 1.66732 to 1.66646, saving model to weights-improvement-48-1.6665.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "97635/97635 [==============================] - 81s 826us/step - loss: 1.6672\n",
      "\n",
      "Epoch 00049: loss did not improve from 1.66646\n",
      "Epoch 50/200\n",
      "97635/97635 [==============================] - 89s 915us/step - loss: 1.6661\n",
      "\n",
      "Epoch 00050: loss improved from 1.66646 to 1.66611, saving model to weights-improvement-50-1.6661.hdf5\n",
      "Epoch 51/200\n",
      "97635/97635 [==============================] - 85s 873us/step - loss: 1.6627\n",
      "\n",
      "Epoch 00051: loss improved from 1.66611 to 1.66269, saving model to weights-improvement-51-1.6627.hdf5\n",
      "Epoch 52/200\n",
      "97635/97635 [==============================] - 77s 793us/step - loss: 1.6621\n",
      "\n",
      "Epoch 00052: loss improved from 1.66269 to 1.66212, saving model to weights-improvement-52-1.6621.hdf5\n",
      "Epoch 53/200\n",
      "97635/97635 [==============================] - 82s 840us/step - loss: 1.6589\n",
      "\n",
      "Epoch 00053: loss improved from 1.66212 to 1.65889, saving model to weights-improvement-53-1.6589.hdf5\n",
      "Epoch 54/200\n",
      "97635/97635 [==============================] - 102s 1ms/step - loss: 1.6646\n",
      "\n",
      "Epoch 00054: loss did not improve from 1.65889\n",
      "Epoch 55/200\n",
      "97635/97635 [==============================] - 91s 936us/step - loss: 1.6566\n",
      "\n",
      "Epoch 00055: loss improved from 1.65889 to 1.65661, saving model to weights-improvement-55-1.6566.hdf5\n",
      "Epoch 56/200\n",
      "97635/97635 [==============================] - 84s 857us/step - loss: 1.6577\n",
      "\n",
      "Epoch 00056: loss did not improve from 1.65661\n",
      "Epoch 57/200\n",
      "97635/97635 [==============================] - 76s 783us/step - loss: 1.6552\n",
      "\n",
      "Epoch 00057: loss improved from 1.65661 to 1.65517, saving model to weights-improvement-57-1.6552.hdf5\n",
      "Epoch 58/200\n",
      "97635/97635 [==============================] - 74s 760us/step - loss: 1.6552\n",
      "\n",
      "Epoch 00058: loss did not improve from 1.65517\n",
      "Epoch 59/200\n",
      "97635/97635 [==============================] - 77s 786us/step - loss: 1.6552\n",
      "\n",
      "Epoch 00059: loss did not improve from 1.65517\n",
      "Epoch 60/200\n",
      "97635/97635 [==============================] - 81s 827us/step - loss: 1.6567\n",
      "\n",
      "Epoch 00060: loss did not improve from 1.65517\n",
      "Epoch 61/200\n",
      "97635/97635 [==============================] - 83s 846us/step - loss: 1.6545\n",
      "\n",
      "Epoch 00061: loss improved from 1.65517 to 1.65454, saving model to weights-improvement-61-1.6545.hdf5\n",
      "Epoch 62/200\n",
      "97635/97635 [==============================] - 83s 853us/step - loss: 1.6510\n",
      "\n",
      "Epoch 00062: loss improved from 1.65454 to 1.65100, saving model to weights-improvement-62-1.6510.hdf5\n",
      "Epoch 63/200\n",
      "97635/97635 [==============================] - 81s 830us/step - loss: 1.6543\n",
      "\n",
      "Epoch 00063: loss did not improve from 1.65100\n",
      "Epoch 64/200\n",
      "97635/97635 [==============================] - 75s 767us/step - loss: 1.6457\n",
      "\n",
      "Epoch 00064: loss improved from 1.65100 to 1.64572, saving model to weights-improvement-64-1.6457.hdf5\n",
      "Epoch 65/200\n",
      "97635/97635 [==============================] - 77s 794us/step - loss: 1.6479\n",
      "\n",
      "Epoch 00065: loss did not improve from 1.64572\n",
      "Epoch 66/200\n",
      "97635/97635 [==============================] - 80s 820us/step - loss: 1.6469ETA: 3s - ETA: \n",
      "\n",
      "Epoch 00066: loss did not improve from 1.64572\n",
      "Epoch 67/200\n",
      "97635/97635 [==============================] - 75s 769us/step - loss: 1.6485\n",
      "\n",
      "Epoch 00067: loss did not improve from 1.64572\n",
      "Epoch 68/200\n",
      "97635/97635 [==============================] - 80s 820us/step - loss: 1.6441\n",
      "\n",
      "Epoch 00068: loss improved from 1.64572 to 1.64414, saving model to weights-improvement-68-1.6441.hdf5\n",
      "Epoch 69/200\n",
      "97635/97635 [==============================] - 79s 811us/step - loss: 1.6488\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.64414\n",
      "Epoch 70/200\n",
      "97635/97635 [==============================] - 86s 883us/step - loss: 1.6409\n",
      "\n",
      "Epoch 00070: loss improved from 1.64414 to 1.64089, saving model to weights-improvement-70-1.6409.hdf5\n",
      "Epoch 71/200\n",
      "97635/97635 [==============================] - 89s 911us/step - loss: 1.63611s - \n",
      "\n",
      "Epoch 00071: loss improved from 1.64089 to 1.63607, saving model to weights-improvement-71-1.6361.hdf5\n",
      "Epoch 72/200\n",
      "97635/97635 [==============================] - 88s 899us/step - loss: 1.6394\n",
      "\n",
      "Epoch 00072: loss did not improve from 1.63607\n",
      "Epoch 73/200\n",
      "97635/97635 [==============================] - 85s 866us/step - loss: 1.6439\n",
      "\n",
      "Epoch 00073: loss did not improve from 1.63607\n",
      "Epoch 74/200\n",
      "97635/97635 [==============================] - 83s 847us/step - loss: 1.6422\n",
      "\n",
      "Epoch 00074: loss did not improve from 1.63607\n",
      "Epoch 75/200\n",
      "97635/97635 [==============================] - 93s 951us/step - loss: 1.6301\n",
      "\n",
      "Epoch 00075: loss improved from 1.63607 to 1.63015, saving model to weights-improvement-75-1.6301.hdf5\n",
      "Epoch 76/200\n",
      "97635/97635 [==============================] - 80s 816us/step - loss: 1.6345\n",
      "\n",
      "Epoch 00076: loss did not improve from 1.63015\n",
      "Epoch 77/200\n",
      "97635/97635 [==============================] - 78s 801us/step - loss: 1.6336\n",
      "\n",
      "Epoch 00079: loss did not improve from 1.63015\n",
      "Epoch 80/200\n",
      "97635/97635 [==============================] - 77s 793us/step - loss: 1.6311\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.63015\n",
      "Epoch 81/200\n",
      "97635/97635 [==============================] - 90s 918us/step - loss: 1.6279\n",
      "\n",
      "Epoch 00081: loss improved from 1.63015 to 1.62786, saving model to weights-improvement-81-1.6279.hdf5\n",
      "Epoch 82/200\n",
      "97635/97635 [==============================] - 130s 1ms/step - loss: 1.6288\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.62786\n",
      "Epoch 84/200\n",
      "97635/97635 [==============================] - 116s 1ms/step - loss: 1.6320\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.62786\n",
      "Epoch 86/200\n",
      "97635/97635 [==============================] - 113s 1ms/step - loss: 1.6328\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.62786\n",
      "Epoch 87/200\n",
      "97635/97635 [==============================] - 115s 1ms/step - loss: 1.6261\n",
      "\n",
      "Epoch 00088: loss did not improve from 1.62211\n",
      "Epoch 89/200\n",
      "97635/97635 [==============================] - 111s 1ms/step - loss: 1.6243\n",
      "\n",
      "Epoch 00091: loss did not improve from 1.62029\n",
      "Epoch 92/200\n",
      "97635/97635 [==============================] - 96s 983us/step - loss: 1.6154\n",
      "\n",
      "Epoch 00094: loss improved from 1.61554 to 1.61544, saving model to weights-improvement-94-1.6154.hdf5\n",
      "Epoch 95/200\n",
      "97635/97635 [==============================] - 106s 1ms/step - loss: 1.6108\n",
      "\n",
      "Epoch 00100: loss improved from 1.61192 to 1.61080, saving model to weights-improvement-100-1.6108.hdf5\n",
      "Epoch 101/200\n",
      "97635/97635 [==============================] - 115s 1ms/step - loss: 1.6096\n",
      "\n",
      "Epoch 00104: loss did not improve from 1.60923\n",
      "Epoch 105/200\n",
      "97635/97635 [==============================] - 111s 1ms/step - loss: 1.6111\n",
      "\n",
      "Epoch 00105: loss did not improve from 1.60923\n",
      "Epoch 106/200\n",
      "97635/97635 [==============================] - 110s 1ms/step - loss: 1.6086\n",
      "\n",
      "Epoch 00106: loss improved from 1.60923 to 1.60863, saving model to weights-improvement-106-1.6086.hdf5\n",
      "Epoch 107/200\n",
      "97635/97635 [==============================] - 88s 905us/step - loss: 1.6043\n",
      "\n",
      "Epoch 00108: loss improved from 1.60863 to 1.60428, saving model to weights-improvement-108-1.6043.hdf5\n",
      "Epoch 109/200\n",
      "97635/97635 [==============================] - 99s 1ms/step - loss: 1.6051\n",
      "\n",
      "Epoch 00109: loss did not improve from 1.60428\n",
      "Epoch 110/200\n",
      "97635/97635 [==============================] - 87s 893us/step - loss: 1.6008\n",
      "\n",
      "Epoch 00115: loss improved from 1.60327 to 1.60085, saving model to weights-improvement-115-1.6008.hdf5\n",
      "Epoch 116/200\n",
      "97635/97635 [==============================] - 93s 953us/step - loss: 1.5972\n",
      "\n",
      "Epoch 00116: loss improved from 1.60085 to 1.59717, saving model to weights-improvement-116-1.5972.hdf5\n",
      "Epoch 117/200\n",
      "97635/97635 [==============================] - 83s 852us/step - loss: 1.6001\n",
      "\n",
      "Epoch 00121: loss did not improve from 1.59375\n",
      "Epoch 122/200\n",
      "97635/97635 [==============================] - 85s 866us/step - loss: 1.5991\n",
      "\n",
      "Epoch 00124: loss did not improve from 1.59375\n",
      "Epoch 125/200\n",
      "97635/97635 [==============================] - 83s 852us/step - loss: 1.5912\n",
      "\n",
      "Epoch 00128: loss did not improve from 1.58721\n",
      "Epoch 129/200\n",
      "97635/97635 [==============================] - 85s 871us/step - loss: 1.5911\n",
      "\n",
      "Epoch 00129: loss did not improve from 1.58721\n",
      "Epoch 130/200\n",
      "97635/97635 [==============================] - 84s 860us/step - loss: 1.5879\n",
      "\n",
      "Epoch 00131: loss did not improve from 1.58721\n",
      "Epoch 132/200\n",
      "97635/97635 [==============================] - 85s 869us/step - loss: 1.5906\n",
      "\n",
      "Epoch 00132: loss did not improve from 1.58721\n",
      "Epoch 133/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97635/97635 [==============================] - 84s 862us/step - loss: 1.5896\n",
      "\n",
      "Epoch 00133: loss did not improve from 1.58721\n",
      "Epoch 134/200\n",
      "97635/97635 [==============================] - 84s 861us/step - loss: 1.5905\n",
      "\n",
      "Epoch 00134: loss did not improve from 1.58721\n",
      "Epoch 135/200\n",
      "97635/97635 [==============================] - 84s 860us/step - loss: 1.5849\n",
      "\n",
      "Epoch 00135: loss improved from 1.58721 to 1.58494, saving model to weights-improvement-135-1.5849.hdf5\n",
      "Epoch 136/200\n",
      "97635/97635 [==============================] - 85s 875us/step - loss: 1.5891\n",
      "\n",
      "Epoch 00136: loss did not improve from 1.58494\n",
      "Epoch 137/200\n",
      "97635/97635 [==============================] - 86s 877us/step - loss: 1.5838\n",
      "\n",
      "Epoch 00137: loss improved from 1.58494 to 1.58383, saving model to weights-improvement-137-1.5838.hdf5\n",
      "Epoch 138/200\n",
      "97635/97635 [==============================] - 88s 900us/step - loss: 1.5816\n",
      "\n",
      "Epoch 00143: loss did not improve from 1.57950\n",
      "Epoch 144/200\n",
      "97635/97635 [==============================] - 92s 944us/step - loss: 1.5837\n",
      "\n",
      "Epoch 00144: loss did not improve from 1.57950\n",
      "Epoch 145/200\n",
      "97635/97635 [==============================] - 78s 798us/step - loss: 1.5773\n",
      "\n",
      "Epoch 00147: loss did not improve from 1.57601\n",
      "Epoch 148/200\n",
      "97635/97635 [==============================] - 91s 936us/step - loss: 1.5726\n",
      "\n",
      "Epoch 00150: loss improved from 1.57493 to 1.57262, saving model to weights-improvement-150-1.5726.hdf5\n",
      "Epoch 151/200\n",
      "97635/97635 [==============================] - 87s 888us/step - loss: 1.5768\n",
      "\n",
      "Epoch 00151: loss did not improve from 1.57262\n",
      "Epoch 152/200\n",
      "97635/97635 [==============================] - 87s 886us/step - loss: 1.5766\n",
      "\n",
      "Epoch 00152: loss did not improve from 1.57262\n",
      "Epoch 153/200\n",
      "97635/97635 [==============================] - 91s 928us/step - loss: 1.5714\n",
      "\n",
      "Epoch 00154: loss improved from 1.57262 to 1.57142, saving model to weights-improvement-154-1.5714.hdf5\n",
      "Epoch 155/200\n",
      "97635/97635 [==============================] - 90s 920us/step - loss: 1.5784\n",
      "\n",
      "Epoch 00155: loss did not improve from 1.57142\n",
      "Epoch 156/200\n",
      "97635/97635 [==============================] - 94s 958us/step - loss: 1.5733\n",
      "\n",
      "Epoch 00156: loss did not improve from 1.57142\n",
      "Epoch 157/200\n",
      "97635/97635 [==============================] - 92s 946us/step - loss: 1.5673\n",
      "\n",
      "Epoch 00157: loss improved from 1.57142 to 1.56726, saving model to weights-improvement-157-1.5673.hdf5\n",
      "Epoch 158/200\n",
      "97635/97635 [==============================] - 85s 873us/step - loss: 1.5745\n",
      "\n",
      "Epoch 00158: loss did not improve from 1.56726\n",
      "Epoch 159/200\n",
      "97635/97635 [==============================] - 85s 867us/step - loss: 1.5704\n",
      "\n",
      "Epoch 00159: loss did not improve from 1.56726\n",
      "Epoch 160/200\n",
      "97635/97635 [==============================] - 84s 856us/step - loss: 1.5723\n",
      "\n",
      "Epoch 00160: loss did not improve from 1.56726\n",
      "Epoch 161/200\n",
      "97635/97635 [==============================] - 85s 870us/step - loss: 1.5674\n",
      "\n",
      "Epoch 00161: loss did not improve from 1.56726\n",
      "Epoch 162/200\n",
      "97635/97635 [==============================] - 96s 986us/step - loss: 1.5752\n",
      "\n",
      "Epoch 00162: loss did not improve from 1.56726\n",
      "Epoch 163/200\n",
      "97635/97635 [==============================] - 113s 1ms/step - loss: 1.5717\n",
      "\n",
      "Epoch 00163: loss did not improve from 1.56726\n",
      "Epoch 164/200\n",
      "97635/97635 [==============================] - 86s 878us/step - loss: 1.5665\n",
      "\n",
      "Epoch 00164: loss improved from 1.56726 to 1.56652, saving model to weights-improvement-164-1.5665.hdf5\n",
      "Epoch 165/200\n",
      "97635/97635 [==============================] - 82s 840us/step - loss: 1.5708\n",
      "\n",
      "Epoch 00165: loss did not improve from 1.56652\n",
      "Epoch 166/200\n",
      "97635/97635 [==============================] - 90s 922us/step - loss: 1.5715\n",
      "\n",
      "Epoch 00166: loss did not improve from 1.56652\n",
      "Epoch 167/200\n",
      "97635/97635 [==============================] - 83s 850us/step - loss: 1.5694\n",
      "\n",
      "Epoch 00167: loss did not improve from 1.56652\n",
      "Epoch 168/200\n",
      "97635/97635 [==============================] - 79s 812us/step - loss: 1.5650\n",
      "\n",
      "Epoch 00168: loss improved from 1.56652 to 1.56501, saving model to weights-improvement-168-1.5650.hdf5\n",
      "Epoch 169/200\n",
      "97635/97635 [==============================] - 89s 911us/step - loss: 1.5641\n",
      "\n",
      "Epoch 00169: loss improved from 1.56501 to 1.56413, saving model to weights-improvement-169-1.5641.hdf5\n",
      "Epoch 170/200\n",
      "97635/97635 [==============================] - 89s 910us/step - loss: 1.5638\n",
      "\n",
      "Epoch 00170: loss improved from 1.56413 to 1.56382, saving model to weights-improvement-170-1.5638.hdf5\n",
      "Epoch 171/200\n",
      "97635/97635 [==============================] - 79s 811us/step - loss: 1.5652\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.56382\n",
      "Epoch 172/200\n",
      "97635/97635 [==============================] - 81s 829us/step - loss: 1.5622\n",
      "\n",
      "Epoch 00172: loss improved from 1.56382 to 1.56216, saving model to weights-improvement-172-1.5622.hdf5\n",
      "Epoch 173/200\n",
      "97635/97635 [==============================] - 82s 842us/step - loss: 1.5620\n",
      "\n",
      "Epoch 00173: loss improved from 1.56216 to 1.56195, saving model to weights-improvement-173-1.5620.hdf5\n",
      "Epoch 174/200\n",
      "97635/97635 [==============================] - 97s 991us/step - loss: 1.5681\n",
      "\n",
      "Epoch 00174: loss did not improve from 1.56195\n",
      "Epoch 175/200\n",
      "97635/97635 [==============================] - 81s 832us/step - loss: 1.5625\n",
      "\n",
      "Epoch 00175: loss did not improve from 1.56195\n",
      "Epoch 176/200\n",
      "97635/97635 [==============================] - 75s 768us/step - loss: 1.5628\n",
      "\n",
      "Epoch 00176: loss did not improve from 1.56195\n",
      "Epoch 177/200\n",
      "97635/97635 [==============================] - 72s 736us/step - loss: 1.5639\n",
      "\n",
      "Epoch 00177: loss did not improve from 1.56195\n",
      "Epoch 178/200\n",
      "97635/97635 [==============================] - 72s 736us/step - loss: 1.5590\n",
      "\n",
      "Epoch 00178: loss improved from 1.56195 to 1.55895, saving model to weights-improvement-178-1.5590.hdf5\n",
      "Epoch 179/200\n",
      "97635/97635 [==============================] - 73s 744us/step - loss: 1.5526\n",
      "\n",
      "Epoch 00179: loss improved from 1.55895 to 1.55259, saving model to weights-improvement-179-1.5526.hdf5\n",
      "Epoch 180/200\n",
      "97635/97635 [==============================] - 72s 739us/step - loss: 1.5601\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.55259\n",
      "Epoch 181/200\n",
      "97635/97635 [==============================] - 87s 895us/step - loss: 1.5594\n",
      "\n",
      "Epoch 00181: loss did not improve from 1.55259\n",
      "Epoch 182/200\n",
      "97635/97635 [==============================] - 83s 852us/step - loss: 1.5600\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.55259\n",
      "Epoch 183/200\n",
      "97635/97635 [==============================] - 81s 830us/step - loss: 1.5603\n",
      "\n",
      "Epoch 00183: loss did not improve from 1.55259\n",
      "Epoch 184/200\n",
      "97635/97635 [==============================] - 84s 862us/step - loss: 1.5591\n",
      "\n",
      "Epoch 00184: loss did not improve from 1.55259\n",
      "Epoch 185/200\n",
      "97635/97635 [==============================] - 77s 793us/step - loss: 1.5566\n",
      "\n",
      "Epoch 00185: loss did not improve from 1.55259\n",
      "Epoch 186/200\n",
      "97635/97635 [==============================] - 75s 767us/step - loss: 1.5530\n",
      "\n",
      "Epoch 00186: loss did not improve from 1.55259\n",
      "Epoch 187/200\n",
      "97635/97635 [==============================] - 74s 760us/step - loss: 1.5592\n",
      "\n",
      "Epoch 00187: loss did not improve from 1.55259\n",
      "Epoch 188/200\n",
      "97635/97635 [==============================] - 84s 858us/step - loss: 1.5537\n",
      "\n",
      "Epoch 00188: loss did not improve from 1.55259\n",
      "Epoch 189/200\n",
      "97635/97635 [==============================] - 83s 848us/step - loss: 1.5531\n",
      "\n",
      "Epoch 00189: loss did not improve from 1.55259\n",
      "Epoch 190/200\n",
      "97635/97635 [==============================] - 83s 849us/step - loss: 1.5506\n",
      "\n",
      "Epoch 00190: loss improved from 1.55259 to 1.55057, saving model to weights-improvement-190-1.5506.hdf5\n",
      "Epoch 191/200\n",
      "97635/97635 [==============================] - 83s 846us/step - loss: 1.5573\n",
      "\n",
      "Epoch 00191: loss did not improve from 1.55057\n",
      "Epoch 192/200\n",
      "97635/97635 [==============================] - 83s 852us/step - loss: 1.5473\n",
      "\n",
      "Epoch 00192: loss improved from 1.55057 to 1.54735, saving model to weights-improvement-192-1.5473.hdf5\n",
      "Epoch 193/200\n",
      "97635/97635 [==============================] - 85s 873us/step - loss: 1.5579\n",
      "\n",
      "Epoch 00193: loss did not improve from 1.54735\n",
      "Epoch 194/200\n",
      "97635/97635 [==============================] - 83s 849us/step - loss: 1.5533\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.54735\n",
      "Epoch 195/200\n",
      "97635/97635 [==============================] - 76s 776us/step - loss: 1.54790s - loss\n",
      "\n",
      "Epoch 00195: loss did not improve from 1.54735\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97635/97635 [==============================] - 78s 797us/step - loss: 1.5459\n",
      "\n",
      "Epoch 00196: loss improved from 1.54735 to 1.54595, saving model to weights-improvement-196-1.5459.hdf5\n",
      "Epoch 197/200\n",
      "97635/97635 [==============================] - 74s 757us/step - loss: 1.5530\n",
      "\n",
      "Epoch 00197: loss did not improve from 1.54595\n",
      "Epoch 198/200\n",
      "97635/97635 [==============================] - 74s 755us/step - loss: 1.5535\n",
      "\n",
      "Epoch 00198: loss did not improve from 1.54595\n",
      "Epoch 199/200\n",
      "97635/97635 [==============================] - 76s 778us/step - loss: 1.5495\n",
      "\n",
      "Epoch 00199: loss did not improve from 1.54595\n",
      "Epoch 200/200\n",
      "97635/97635 [==============================] - 76s 781us/step - loss: 1.5456\n",
      "\n",
      "Epoch 00200: loss improved from 1.54595 to 1.54558, saving model to weights-improvement-200-1.5456.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x173999b9b00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(X, y, epochs=200, batch_size=128, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
